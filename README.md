# ğŸ§  **Haipai-50M-EMA â€” A Readable, Modern GPT-Style Transformer**

**Haipai** (â€œhigh-pieâ€) is a compact, fully readable **decoder-only Transformer** you can understand, train, and extend yourself.
Itâ€™s designed to behave like a scaled-down GPT-style model â€” but remain small enough to fit comfortably on a single modern GPU (or even CPU inference).

The model implements the same modern components as todayâ€™s large LMs â€” **RMSNorm**, **SwiGLU**, **Rotary Position Embeddings (RoPE)**, **causal SDPA attention**, **tied embeddings**, and **EMA-smoothed weights** â€” in about 200 lines of PyTorch.

This release includes:

* âœ… **EMA-averaged weights** in `.safetensors`
* âœ… **Tokenizer** (trained on mixed Wikipedia + CNN/DailyMail)
* âœ… **Minimal PyTorch model implementation**
* âœ… **Inference script** with temperature/top-p sampling

---

## âš¡ Highlights

| Feature             | Description                                       |
| :------------------ | :------------------------------------------------ |
| **Architecture**    | Decoder-only Transformer (GPT-style)              |
| **Modern features** | RMSNorm, SwiGLU, RoPE, SDPA, EMA                  |
| **Parameter count** | ~50M (depending on vocab)                      |
| **Context length**  | 1,024 tokens (RoPE scaled Ã—2)                     |
| **Precision**       | fp16 / bf16                                       |
| **Tokenizer**       | BPE, 50k vocab, lowercase + whitespace            |
| **Export format**   | `.safetensors` (EMA-smoothed)                     |
| **Intended use**    | Pretraining demos, teaching, small-scale research |
| **License**         | (choose: MIT / Apache-2.0 / custom)               |

---

## ğŸ§± Architecture Overview

**HaipaiLM** is a GPT-style decoder block stack with **RMSNorm â†’ Attention â†’ Residual** and **RMSNorm â†’ SwiGLU â†’ Residual**, finalized by an RMSNorm and tied embedding head.

It uses **RoPE** for positional encoding applied to both Q and K, and **causal SDPA** (PyTorch fused attention) for fast inference.

```
Input IDs
   â†“
Embedding
   â†“
[Ã— N blocks]
   â”œâ”€ RMSNorm â†’ Multi-Head Attention (RoPE on QK causal SDPA) â†’ Add Residual
   â””â”€ RMSNorm â†’ SwiGLU â†’ Add Residual
   â†“
Final RMSNorm
   â†“
Tied LM Head (shared with Embedding)
```

---

### ğŸ§© Diagram

This schematic shows how **HaipaiLM** processes tokens:

<img width="938" height="665" alt="diagram-export-11-8-2025-9_11_04-PM" src="https://github.com/user-attachments/assets/39e4b6b1-c889-4c2c-a94e-f8386a0cae26" />


Each block uses:

* **RMSNorm 1 â†’ RoPE â†’ Multi-Head Attention â†’ Residual**
* **RMSNorm 2 â†’ SwiGLU â†’ Residual**
* **Final RMSNorm** normalizes before the output head
* **Embedding â†” LM Head** share the same weights

---

## âš™ï¸ Training Configuration

| Component                    | Setting                           |
| :--------------------------- | :-------------------------------- |
| **Layers**                   | 12                                |
| **Hidden size (d_model)**    | 384                               |
| **Attention heads (n_head)** | 6                                 |
| **Head dimension**           | 64                                |
| **Feed-forward (SwiGLU)**    | 2048                              |
| **Norm**                     | RMSNorm (Îµ=1e-6)                  |
| **Dropout**                  | 0.0â€“0.01                          |
| **RoPE scale**               | 2.0                               |
| **Optimizer**                | AdamW (Î²â‚=0.9, Î²â‚‚=0.95, Îµ=1e-8)   |
| **Weight decay**             | 0.1                               |
| **Gradient clip**            | 0.8                               |
| **LR schedule**              | Linear warmup â†’ Cosine decay      |
| **Warmup steps**             | 1000                              |
| **Max LR**                   | 3e-3                              |
| **EMA decay**                | 0.9995                            |
| **Precision**                | AMP (bf16/fp16)                   |
| **Objective**                | Causal LM (next-token prediction) |

---

## ğŸ“Š Training Recipe Summary

**Dataset mix:**

* 60% Wikipedia (latest English dump)
* 40% CNN/DailyMail (news articles)
* ~200â€“500M total tokens

**Tokenizer:**

* Trained BPE (50k vocab)
* Lowercased, whitespace pre-tokenized
* Saved in `/tokenizer/` folder

**Validation perplexity:**

* ~60 after early stage (1kâ€“2k steps)
* ~25â€“30 after full 500M-token run
* **EMA weights** yield smoother generations

---

## ğŸ“‚ Repository Structure

```
model_safe/
â”œâ”€â”€ modeling_haipai.py        # Full HaipaiLM model definition
â”œâ”€â”€ inference.py              # Inference + sampling CLI
â”œâ”€â”€ config.json               # Model dimensions + metadata
â”œâ”€â”€ model.safetensors         # EMA-smoothed weights
â”œâ”€â”€ tokenizer/
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â””â”€â”€ vocab.json
â””â”€â”€ README.md
```

---

## ğŸ§ª Local Inference

```python
import torch, os, json
from safetensors.torch import load_file
from transformers import AutoTokenizer
from modeling_haipai import HaipaiLM

repo_dir = "./haipai-50m-ema"

# 1. Load config + tokenizer
with open(os.path.join(repo_dir, "config.json")) as f:
    cfg = json.load(f)
tok = AutoTokenizer.from_pretrained(os.path.join(repo_dir, "tokenizer"), local_files_only=True)
if tok.pad_token is None and tok.eos_token is not None:
    tok.pad_token = tok.eos_token

# 2. Load model
state = load_file(os.path.join(repo_dir, "model.safetensors"))
model = HaipaiLM(
    vocab_size=tok.vocab_size,
    d_model=cfg["d_model"], n_layer=cfg["n_layer"], n_head=cfg["n_head"],
    d_ff=cfg["d_ff"], max_seq_len=cfg["max_position_embeddings"],
    rope_scale=cfg.get("rope_scale", 2.0),
).to("cpu").eval()
model.load_state_dict(state, strict=False)

# 3. Generate
prompt = "Doctor told"
ids = tok(prompt, return_tensors="pt").input_ids
with torch.no_grad():
    logits, _ = model(ids)
```

---

## ğŸ’» CLI Inference

```bash
python inference.py --local_dir "./haipai-50m-ema" --prompt "Doctor told" --device cpu
```

Options:

```
--local_dir            Path to model + tokenizer
--prompt               Input prompt
--device               cuda / cpu
--max_new_tokens       (default 120)
--temperature          (default 0.7)
--top_p                (default 0.9)
--repetition_penalty   (default 1.15)
```

---

## ğŸ”¥ Sampling Defaults

```python
temperature = 0.7
top_p = 0.9
repetition_penalty = 1.15
max_new_tokens = 150
```

You can also experiment with:

* Lower temperature (0.6) for factual output
* Higher top_p (0.95) for more variety
* Larger context windows for longer text generation

---

## ğŸ“ˆ Evaluate Perplexity

```python
import math, torch
@torch.no_grad()
def ppl_on_texts(model, tok, texts):
    losses = []
    for text in texts:
        enc = tok(text, return_tensors="pt")
        ids = enc.input_ids
        labels = ids.clone()
        labels[:, :-1] = ids[:, 1:]
        labels[:, -1] = -100
        _, loss = model(ids, labels)
        losses.append(loss.item())
    return math.exp(sum(losses)/len(losses))
```

---

## ğŸ§© Design Notes

**RMSNorm** â€“ lightweight alternative to LayerNorm, numerically stable for small models.
**SwiGLU** â€“ nonlinearity `SiLU(Wâ‚x) âŠ™ Wâ‚‚x â†’ Wâ‚ƒ`, better expressiveness.
**RoPE** â€“ encodes relative positions via sinusoidal rotation of Q/K.
**Causal SDPA** â€“ built-in PyTorch scaled dot-product attention backend (FlashAttention on supported GPUs).
**EMA** â€“ maintains a running exponential average of weights during training, improving validation loss and sample smoothness.

---

## âš ï¸ Limitations

* ~50M parameters â€” for research, not production deployment.
* Not instruction-tuned; wonâ€™t follow chat-style prompts.
* Quality depends on corpus balance (Wikipedia + news).
* May produce factual or stylistic inconsistencies.
  Use responsibly with human review.

---

## ğŸ§­ Extending Haipai

You can scale the model easily:

* 100M: double `d_model` and `d_ff`
* 1B+: use Mixture-of-Experts (MoE) for efficient scaling
* Replace `HaipaiBlock` with sparse MoE or gated variants
* Try longer context windows by rebuilding RoPE cache

---

## ğŸ“˜ Citation

```bibtex
@software{haipai2025,
  title   = {Haipai: A Minimal, Modern GPT-Style Language Model (~50M, EMA)},
  author  = {Md Rakibul Islam Rocky},
  year    = {2025},
  url     = {https://huggingface.co/rocky1410/haipai-nano}
}
```

---

## âš–ï¸ License

This repository is open for research and educational use on Apache 2.0 License

---
